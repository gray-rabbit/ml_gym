{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gym pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#일단 환경을 만든다.\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state #게임을 reset하면 새로운 state가 반환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env에 step에 액션을 넣어준다.\n",
    "# env.action_space.sample()를 하면 랜덤의 액션이 생성된다.\n",
    "result = env.step(env.action_space.sample())\n",
    "env.render()\n",
    "result #step의 경과로는 다음 상태, reward, 게임의 종류, prob값이 나온다.\n",
    "\n",
    "#죽은 상태에서는 게임이 더 이상 진행되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#게임 환경을 닫는다.\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 게임을 닫은 후에는 다시 렌더링이 되지 않는다.\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#따라서 다음과 같은 논리적 구조를 만들어 여러번 실행이 가능하다.\n",
    "# 아래의 코드는 10판의 게임을 돌리는 코드이다.\n",
    "for i in range(10):\n",
    "    game_over= False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    while not game_over:\n",
    "        next_state, reward, game_over, _ = env.step(env.action_space.sample())\n",
    "        score += reward\n",
    "        env.render()\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#온전히 랜덤만으로 목표 지점에 도달할수 있을까?\n",
    "# render를 하게 되면 너무 많은 시간이 소요됨으로 render를 시행하지 않는다.\n",
    "for i in range(1000):\n",
    "    game_over= False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    while not game_over:\n",
    "        next_state, reward, game_over, _ = env.step(env.action_space.sample())\n",
    "        score += reward\n",
    "    if score>0:\n",
    "        print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/270F6B3A567D7EA706\" width=\"100px\">\n",
    "\n",
    "이게 되네!!??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',is_slippery=False)\n",
    "#Q-function을 위한 \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#총 2000판을 한다.\n",
    "epi = 3000\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        #최대의 reward를 고를 수 있는 action을 선택\n",
    "        m = np.max(Q[state])\n",
    "        action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "        new_state, reward, game_over, _ = env.step(action)\n",
    "        #Q를 업데이트 한다.\n",
    "        Q[state, action] = reward + np.max(Q[new_state])\n",
    "        state = new_state\n",
    "    \n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        print(Q[i*4+j],end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table을 이용해 게임을 플레이 해보자.\n",
    "env = gym.make('FrozenLake-v1',is_slippery=False)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "    #최대의 reward를 고를 수 있는 action을 선택\n",
    "    m = np.max(Q[state])\n",
    "    action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "    new_state, reward, game_over, _ = env.step(action)\n",
    "    env.render()\n",
    "    #Q를 업데이트 한다.\n",
    "    Q[state, action] = reward + np.max(Q[new_state])\n",
    "    state = new_state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#더 큰 게임환경에서도 가능할까?\n",
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)\n",
    "#Q-function을 위한 \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#총 2000판을 한다.\n",
    "epi = 3000\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        #최대의 reward를 고를 수 있는 action을 선택\n",
    "        m = np.max(Q[state])\n",
    "        action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "        new_state, reward, game_over, _ = env.step(action)\n",
    "        #Q를 업데이트 한다.\n",
    "        Q[state, action] = reward + np.max(Q[new_state])\n",
    "        state = new_state\n",
    "    \n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        print(Q[i*4+j],end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table을 이용해 게임을 플레이 해보자.\n",
    "env = gym.make('FrozenLake-v1',map_name=\"8x8\",is_slippery=False)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "    #최대의 reward를 고를 수 있는 action을 선택\n",
    "    m = np.max(Q[state])\n",
    "    action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "    new_state, reward, game_over, _ = env.step(action)\n",
    "    env.render()\n",
    "    #Q를 업데이트 한다.\n",
    "    Q[state, action] = reward + np.max(Q[new_state])\n",
    "    state = new_state\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뭔가...이상한데???\n",
    "# 왜 이상한 길로 갈까?\n",
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',map_name=\"8x8\", is_slippery=False)\n",
    "#Q-function을 위한 \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#총 2000판을 한다.\n",
    "epi = 5000\n",
    "gamma = .9\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        #최대의 reward를 고를 수 있는 action을 선택\n",
    "        m = np.max(Q[state])\n",
    "        action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "        new_state, reward, game_over, _ = env.step(action)\n",
    "        #Q를 업데이트 한다.\n",
    "        Q[state, action] = reward + gamma*np.max(Q[new_state])\n",
    "        state = new_state\n",
    "    \n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        print(Q[i*4+j],end=\"\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table을 이용해 게임을 플레이 해보자.\n",
    "env = gym.make('FrozenLake-v1',map_name=\"8x8\",is_slippery=False)\n",
    "state = env.reset()\n",
    "env.render()\n",
    "game_over = False\n",
    "while not game_over:\n",
    "    #최대의 reward를 고를 수 있는 action을 선택\n",
    "    m = np.max(Q[state])\n",
    "    action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "    new_state, reward, game_over, _ = env.step(action)\n",
    "    env.render()\n",
    "    #Q를 업데이트 한다.\n",
    "    Q[state, action] = reward + np.max(Q[new_state])\n",
    "    state = new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미끄러운 환경에서는 작동할까?\n",
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\", is_slippery=True)\n",
    "#Q-function을 위한 \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#총 2000판을 한다.\n",
    "epi = 5000\n",
    "gamma = .9\n",
    "score_list = []\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    score = 0\n",
    "    while not game_over:\n",
    "        #최대의 reward를 고를 수 있는 action을 선택\n",
    "        m = np.max(Q[state])\n",
    "        action = np.random.choice(np.where(Q[state]>=m)[0])\n",
    "        new_state, reward, game_over, _ = env.step(action)\n",
    "        #Q를 업데이트 한다.\n",
    "        Q[state, action] = reward + gamma*np.max(Q[new_state])\n",
    "        state = new_state\n",
    "        score += reward\n",
    "    score_list.append(score)\n",
    "print(\"평균:\",np.average(score_list))\n",
    "n = int(np.sqrt(env.observation_space.n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\",is_slippery=True)\n",
    "#Q-function을 위한 \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "#총 2000판을 한다.\n",
    "epi , gamma,all_reward= 4000,.9,[]\n",
    "lr = .9\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    game_over = False\n",
    "    while not game_over:\n",
    "        #최대의 reward를 고를 수 있는 action을 선택\n",
    "        action = np.argmax(Q[state]+np.random.rand(4)/(i+1))    \n",
    "        new_state, reward, game_over, _ = env.step(action)\n",
    "        #Q를 업데이트 한다.\n",
    "        if state!=new_state:\n",
    "            Q[state, action] = (1-lr)*Q[state,action] + lr*(reward + gamma*np.max(Q[new_state]))\n",
    "        state = new_state\n",
    "    all_reward.append(reward)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.bar(range(epi), all_reward)\n",
    "print(np.sum(all_reward)/epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1',map_name=\"4x4\")\n",
    "game_over = False\n",
    "state = env.reset()\n",
    "while not game_over:\n",
    "    env.render()\n",
    "    action = np.argmax(Q[state])\n",
    "    new_state, reward, game_over, _ = env.step(action)\n",
    "    state = new_state\n",
    "    if game_over:\n",
    "        print(\"game_over\")\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\python_workspace\\ml_gym\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (env.observation_space.high-env.observation_space.low)/20\n",
    "def get_state(state):\n",
    "    restate = (state-env.observation_space.low)/size\n",
    "    pos, vel = restate.astype(\"int16\")\n",
    "    return (pos,vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epi:0 score:-200.0\n",
      "epi:1000 score:0\n",
      "epi:2000 score:0\n",
      "epi:3000 score:0\n",
      "epi:4000 score:0\n"
     ]
    }
   ],
   "source": [
    "epi = 5000\n",
    "gamma = .9\n",
    "lr = .9\n",
    "Q = np.random.uniform(low=-2, high=0, size=(20,20,3))\n",
    "for i in range(epi):\n",
    "    state = env.reset()\n",
    "    \n",
    "    score = 0\n",
    "    game_over=False\n",
    "    while not game_over:\n",
    "        pos,vel = get_state(state)    \n",
    "        action = np.argmax(Q[pos,vel])\n",
    "        new_state, reward, game_over,_  = env.step(action)\n",
    "        new_pos, new_vel = get_state(new_state)\n",
    "        score += reward        \n",
    "        if not game_over:\n",
    "            Q[pos,vel,action] = (1-lr)*Q[pos,vel,action] + lr*(reward+gamma*np.max(Q[new_pos,new_vel]))\n",
    "        elif new_state[0]>=env.goal_position and game_over:\n",
    "            Q[pos,vel,action] = 0\n",
    "            score = 0\n",
    "        state = new_state   \n",
    "    if i%1000==0:\n",
    "        print(f\"epi:{i} score:{score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8,10,0\n",
      "8,9,0\n",
      "8,9,0\n",
      "8,9,0\n",
      "8,9,0\n",
      "8,8,1\n",
      "8,8,1\n",
      "8,8,1\n",
      "8,8,1\n",
      "7,8,2\n",
      "7,8,2\n",
      "7,8,2\n",
      "7,8,2\n",
      "7,8,2\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "7,9,1\n",
      "6,9,2\n",
      "6,9,2\n",
      "6,9,2\n",
      "6,9,2\n",
      "6,10,2\n",
      "6,10,2\n",
      "6,10,2\n",
      "7,10,2\n",
      "7,10,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "7,11,2\n",
      "8,12,2\n",
      "8,12,2\n",
      "8,12,2\n",
      "8,12,2\n",
      "8,12,2\n",
      "8,12,2\n",
      "9,12,2\n",
      "9,12,2\n",
      "9,12,2\n",
      "9,12,2\n",
      "9,12,2\n",
      "10,12,0\n",
      "10,11,0\n",
      "10,11,0\n",
      "10,10,0\n",
      "10,10,0\n",
      "10,10,0\n",
      "10,9,0\n",
      "10,9,0\n",
      "10,9,0\n",
      "10,8,0\n",
      "10,8,0\n",
      "9,7,0\n",
      "9,7,0\n",
      "9,7,0\n",
      "9,6,0\n",
      "8,6,0\n",
      "8,6,0\n",
      "8,6,0\n",
      "8,5,0\n",
      "7,5,0\n",
      "7,5,0\n",
      "6,5,1\n",
      "6,5,1\n",
      "6,5,1\n",
      "5,5,0\n",
      "5,5,0\n",
      "5,5,0\n",
      "4,5,1\n",
      "4,5,1\n",
      "4,6,0\n",
      "4,6,0\n",
      "3,6,0\n",
      "3,6,0\n",
      "3,6,0\n",
      "2,6,1\n",
      "2,7,1\n",
      "2,7,1\n",
      "2,8,0\n",
      "2,8,0\n",
      "2,8,0\n",
      "2,8,0\n",
      "1,8,0\n",
      "1,9,0\n",
      "1,9,0\n",
      "1,9,0\n",
      "1,9,0\n",
      "1,9,0\n",
      "1,10,2\n",
      "1,10,2\n",
      "1,11,2\n",
      "2,11,2\n",
      "2,12,1\n",
      "2,12,1\n",
      "2,12,1\n",
      "2,13,1\n",
      "3,13,1\n",
      "3,13,1\n",
      "3,14,2\n",
      "4,14,2\n",
      "4,15,1\n",
      "4,15,1\n",
      "5,15,2\n",
      "5,15,2\n",
      "6,16,0\n",
      "6,16,0\n",
      "7,16,2\n",
      "7,16,2\n",
      "8,16,2\n",
      "8,16,2\n",
      "9,16,2\n",
      "9,16,2\n",
      "10,16,2\n",
      "10,16,2\n",
      "11,16,2\n",
      "11,15,0\n",
      "12,15,2\n",
      "12,15,2\n",
      "12,15,2\n",
      "13,14,2\n",
      "13,14,2\n",
      "13,14,2\n",
      "14,14,1\n",
      "14,13,2\n",
      "14,13,2\n",
      "15,13,2\n",
      "15,13,2\n",
      "15,13,2\n",
      "15,13,2\n",
      "16,12,2\n",
      "16,12,2\n",
      "16,12,2\n",
      "16,12,2\n",
      "17,12,1\n",
      "17,12,1\n",
      "17,12,1\n",
      "17,12,1\n",
      "17,11,2\n",
      "17,11,2\n",
      "17,11,2\n",
      "18,11,2\n",
      "18,12,2\n",
      "18,12,2\n",
      "18,12,2\n",
      "18,12,2\n"
     ]
    }
   ],
   "source": [
    "game_over=False\n",
    "state = env.reset()\n",
    "while not game_over:\n",
    "    env.render()\n",
    "    pos, vel = get_state(state)\n",
    "    action = np.argmax(Q[pos,vel])\n",
    "    print(f\"{pos},{vel},{action}\")\n",
    "    new_state, reward, game_over, _ = env.step(action)\n",
    "    state = new_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras import Sequential, layers, optimizers, losses\n",
    "import gym \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml_gym': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6ac081869b530f457619791f617d77cf805282e7db014faab8a21ecdc256c9d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
